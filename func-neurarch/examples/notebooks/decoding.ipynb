{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "#### Preliminary steps\n",
    "\n",
    "**Note:** For simplicity, we will just use default task and network instanciations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fna.tasks.symbolic import SymbolicSequencer\n",
    "from fna.tasks.symbolic.embeddings import VectorEmbeddings\n",
    "from fna.encoders import NESTEncoder, InputMapper\n",
    "from examples import example_defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize NEST kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 0.1\n",
    "example_defaults.reset_kernel(resolution=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create target network (SNN) - here load a simple default SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[__init__.py:47 - INFO] Initializing Brunel BRN architecture (NEST-simulated)\n",
      "[__init__.py:97 - INFO] Creating populations:\n",
      "[__init__.py:121 - INFO] - Population E, with ids [1-80]\n",
      "[__init__.py:121 - INFO] - Population I, with ids [81-100]\n",
      "[__init__.py:417 - INFO] Connecting Devices: \n",
      "[__init__.py:129 - INFO]   - Attaching spike_detector with gid [(101,)] to population E\n",
      "[__init__.py:438 - INFO] - Connecting spike_detector to E, with label E_spike_detector and id (101,)\n",
      "[__init__.py:129 - INFO]   - Attaching spike_detector with gid [(102,)] to population I\n",
      "[__init__.py:438 - INFO] - Connecting spike_detector to I, with label I_spike_detector and id (102,)\n",
      "[__init__.py:134 - INFO] Randomizing initial values:\n",
      "[__init__.py:95 - INFO] - Randomizing V_m state in Population E\n",
      "[__init__.py:95 - INFO] - Randomizing V_m state in Population I\n",
      "[__init__.py:941 - INFO] ========================================================\n",
      "[__init__.py:942 - INFO]  Brunel BRN architecture (NEST-simulated):\n",
      "[__init__.py:943 - INFO] --------------------------------------------------------\n",
      "[__init__.py:944 - INFO] - 2 Populations: ['E', 'I']\n",
      "[__init__.py:945 - INFO] - Size: 100 [80, 20]\n",
      "[__init__.py:947 - INFO] - Neuron models: ['iaf_psc_delta', 'iaf_psc_delta']\n",
      "[__init__.py:948 - INFO] - [1, 1] Devices connected: [['E_spike_detector'], ['I_spike_detector']]\n",
      "[__init__.py:950 - INFO]     E: ['E_spike_detector'], ['spike_detector']\n",
      "[__init__.py:950 - INFO]     I: ['I_spike_detector'], ['spike_detector']\n",
      "[connectivity.py:187 - INFO] Connecting networks: Brunel BRN -> Brunel BRN\n",
      "[connectivity.py:196 - INFO]     - ('E', 'E') [static_synapse]\n",
      "[connectivity.py:196 - INFO]     - ('E', 'I') [static_synapse]\n",
      "[connectivity.py:196 - INFO]     - ('I', 'E') [static_synapse]\n",
      "[connectivity.py:196 - INFO]     - ('I', 'I') [static_synapse]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading default neuron parameter - iaf_psc_delta, fixed voltage threshold, fixed absolute refractory time\n"
     ]
    }
   ],
   "source": [
    "snn, snn_recurrent_connections = example_defaults.default_network(N=100, record_spikes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an input sequencer and sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sequences.py:132 - INFO] Generating symbolic sequencer\n"
     ]
    }
   ],
   "source": [
    "alphabet_size = 5\n",
    "T = 100\n",
    "\n",
    "sequencer = SymbolicSequencer(label='random', set_size=alphabet_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Create a [vector embedding](embeddings.ipynb#section3.1) (or any other) for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as binary-codeword\n"
     ]
    }
   ],
   "source": [
    "emb = VectorEmbeddings(vocabulary=sequencer.tokens).binary_codeword(dim=100, density=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate discrete stimulus sequence and convert it to continuous input by unfolding the stimulus representations \n",
    "in time (see [unfolding embeddings](embeddings.ipynb#section4.1.2)), or, alternatively, convert each token to a unique \n",
    "spatiotemporal spike pattern (see [frozen noise embeddings](embeddings.ipynb#section4.1.1)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:362 - INFO] Populating Stimulus Set: \n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as continuous signal unfolding\n"
     ]
    }
   ],
   "source": [
    "signal_pars = {'duration': 50.,'amplitude': 150.,'kernel': ('box', {}),'dt': resolution}\n",
    "signal = emb.unfold(to_signal=True, **signal_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Encode stimuli (see [encodings](encodings.ipynb#section6)), the input sequence will be provided later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[__init__.py:35 - INFO] Creating Generators: \n",
      "[__init__.py:38 - INFO] - poisson-input [(103,)-(202,)]\n"
     ]
    }
   ],
   "source": [
    "encoder = NESTEncoder('inhomogeneous_poisson_generator', label='poisson-input', dim=emb.embedding_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Connect encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[__init__.py:116 - INFO] Connecting input generators: poisson-input -> Brunel BRN\n",
      "[__init__.py:127 - INFO]     - ('E', 'poisson-input') [static_synapse]\n",
      "[__init__.py:127 - INFO]     - ('I', 'poisson-input') [static_synapse]\n"
     ]
    }
   ],
   "source": [
    "# input synapses\n",
    "input_synapses = {\n",
    "    'connect_populations': [('E', encoder.name), ('I', encoder.name), ],\n",
    "    'weight_matrix': [None, None],\n",
    "    'conn_specs': [{'rule': 'all_to_all'}, {'rule': 'all_to_all'}],\n",
    "    'syn_specs': [{'model': 'static_synapse', 'delay': 0.1, 'weight': 3.},\n",
    "                  {'model': 'static_synapse', 'delay': 0.1, 'weight': 3.}]\n",
    "}\n",
    "in_to_snn_connections = InputMapper(source=encoder, target=snn, parameters=input_synapses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create and connect [state extractors](state_sampling.ipynb): \n",
    "    * For this example, we will use the simplest sampling strategy whereby the population response vectors are gathered at stimulus offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_parameters = {\n",
    "        'E_Vm_@offset': {\n",
    "            'population': 'E',\n",
    "            'variable': 'V_m',\n",
    "            'sampling_times': ['stim_offset']}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **12. Decoding and Readout - massively parallel information processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "The decoding process consists of combining the input-driven population responses $\\mathbf{x}(t) \\in \\mathbb{R}_{\\mathrm{N}}$ to approximate the desired target outputs $\\mathbf{z}(t) = f(\\mathbf{u}(t)) \\in \\mathbb{R}_{\\mathrm{N_{out}}}$:\n",
    "\n",
    "$$\\mathbf{y}(t) = g (\\mathbf{W}^{\\mathrm{out}} \\mathbf{x}(t) + \\mathbf{b}^{\\mathrm{out}})$$\n",
    "\n",
    "where $\\mathbf{y}(t)$ is an estimate of $\\mathbf{z}(t)$ obtained by (linearly) combining population responses with coefficients $\\mathbf{W}^{\\mathrm{out}}$. Finding $\\mathbf{W_{\\mathrm{out}}}$ amounts to minimizing the error between target output and estimated output:\n",
    "\n",
    "$$E_{\\theta} = \\langle \\mathcal{L}(\\mathbf{y}, \\mathbf{z}) + R_{\\theta}\\rangle_{T}$$\n",
    "\n",
    "to find the decoder parameters $\\theta=\\{\\mathbf{W}^{\\mathrm{out}} \\in \\mathbb{R}_{N\\times N_{\\mathrm{out}}}, \\mathbf{b}^{\\mathrm{out}} \\in \\mathbb{R}_{N}\\}$. \n",
    "\n",
    "To achieve this, we collect the population states $X \\in \\mathbb{R}_{\\mathrm{N\\times T}}$ and the target outputs $Z \\in \\mathbb{R}_{\\mathrm{N_{out}\\times T}}$ for a given data batch of length $T$. During a training phase, we calibrate a supervised (often linear) readout to establish the proper mappings.\n",
    "\n",
    "**Note:** Often the mappings are time-discrete (symbolic), whereas the states are continous. In these cases, we need to adequately sample and observe the continuous dynamics at discrete times ($t^{*}$, see [state extraction](state_sampling.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1. The `Decoder` class and `Readout` algorithms\n",
    "\n",
    "There are several algorithms available. In this example, we will employ a battery of readouts for a single `StateMatrix` on a simple classification task and compare their properties. For this purpose, we specify the parameters of the `Decoder`(s) to connect to the main `SpikingNetwork`. The decoding parameters have the following format: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "decoding_parameters = {\n",
    "    'readout_label': {\n",
    "        'algorithm': 'pinv', # readout algorithm\n",
    "        'extractor': extractor_label # state extractor label\n",
    "        'save': True # store intermediate weights after each training epoch\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All readouts are instantiated via scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.1. Single update readouts\n",
    "\n",
    "```python\n",
    "'pinv', 'ridge', 'logistic', 'perceptron', 'svm-linear', 'svm-rbf' \n",
    "```\n",
    "\n",
    "Some algorithms do not support incremental updates, i.e. **each new batch will overwrite previous results**. These are retained, mostly for legacy reasons, but can only be correctly used if only one training batch is given. Otherwise, each batch is processed independently and the training outcome will refer only to the last batch as all previous results will be overwritten. \n",
    "\n",
    "If operating on a single batch, there are simple straightforward solutions. These algorithms include:\n",
    "\n",
    "* Moore-Penrose direct pseudo-inverse (`pinv`) - quadratic loss function, no regularization\n",
    "\n",
    "* Ridge regression (`ridge`) - quadratic loss function, L2 weight regularization\n",
    "\n",
    "* Logistic regression (`logistic`) - classifier, log-loss, no regularization\n",
    "\n",
    "* Linear SVM (`svm-linear`) - classifier, Huber (soft-margin) loss, with or without regu\n",
    "\n",
    "* Non-linear SVM (`svm-rbf`) - classifier, with radial basis function kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.2. Incremental readouts\n",
    "\n",
    "```python\n",
    "'force', 'sgd-reg', 'sgd-class', 'ridge-sgd', 'pinv-sgd', 'logistic-sgd', 'svm-linear-sgd'\n",
    "```\n",
    "\n",
    "When processing multiple batches of data, readout learning needs to be done incrementally following each batch. To achieve this, we make use of estimators that support `partial_fit` in `sklearn` or use recursive methods (the `FORCE` algorithm implements local recursive least squares). \n",
    "\n",
    "\n",
    "\n",
    "The options currently implemented are:\n",
    "\n",
    "* **FORCE learning (`force`)**: is more effective if the output can drive the system, as modifications are instantaneous. In the absence of output feedback, this method tends to overfit the training data. The output weights are updated as:\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "* **Stochastic Gradient Descent (SGD)**: approximates the error gradient by considering a single training example at a time and incrementally updating the decoder weights $w$ and biases $b$:\n",
    "\n",
    "$\\Delta w = -\\eta\\left(\\alpha \\frac{\\partial R_{w}}{\\partial w}+\\frac{\\partial \\mathcal{L}\\left(w^{T} x_{i}+b, y_{i}\\right)}{\\partial w}\\right)$\n",
    "\n",
    "$\\Delta b = -\\eta\\left(\\alpha \\frac{\\partial \\mathcal{L}\\left(w^{T} x_{i}+b, y_{i}\\right)}{\\partial b}\\right)$\n",
    "    \n",
    "$\\eta$ is the learning rate, which can either be a constant hyperparameter or adaptive (setting the `learning_rate` parameter to `\"optimal\"`, `\"invscaling\"` or `\"adaptive\"` (see [sklearn documentation](https://scikit-learn.org/stable/modules/sgd.html) for more details). By default, and to minimize the number of free parameters, we use the `\"optimal\"` learning rate, whereby \n",
    "\n",
    "$$\\eta(t)=\\frac{1}{\\alpha\\left(t_{0}+t\\right)}$$\n",
    "\n",
    "\n",
    "Depending on the choice of loss function and regularization type, we can implement:\n",
    "   - Least-squares regression (`\"pinv-sgd\"`) - quadratic loss, no regularization ($\\alpha=0$), somewhat similar to recursive least squares\n",
    "   - Ridge regression (`\"ridge-sgd\"`) - quadratic loss, L2 regularization ($\\alpha$ is a free hyper-parameter)\n",
    "   - Logistic regression (`\"logistic-sgd\"`) - Log-loss, with or without L2 regularization ($\\alpha$ is a free hyper-parameter) \n",
    "   - Support Vector Classification (`\"svm-linear-sgd\"`) - Hinge (soft-margin) loss, with or without L2 regularization ($\\alpha$ is a free hyper-parameter)\n",
    "   \n",
    "The above are special cases which are separated in the current implementation, for convenience. However, the `sklearn` implementation of SGD can accomodate a variety of different algorithms for either regression or classification problems, by specifying different combinations of the types of `loss` and `penalty`, as well as the hyperparameter values for $\\alpha$ and $\\eta_{0}$. By setting the decoder to use `\"sgd-reg\"` or `\"sgd-class\"`, the system runs a large search over these combinations to find the best decoder possible (note: for large datasets and large state-spaces, this may be prohibitively costly). \n",
    "\n",
    "\n",
    "**Notes:** There are some peculiarities of using `partial_fit` that may skew the results. The user is advised to pay attention to these:\n",
    "* All hyperparameters are chosen using CV on the first training batch. So, make sure it is representative and large enough.\n",
    "* This is particularly important when training classifiers (\"sgd-class\", \"logistic-sgd\" and \"svm-linear-sgd\"), as they may be unable to cope with unseen examples. To circumvent this, one may need to pass the `classes` argument when calling `partial_fit`\n",
    "* `sgd-reg` and `sgd-class` encompass all other `SGD` objects, which means the hyperparameter search will be long and costly...\n",
    "* Feature scaling and regularization ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2. The `OuputMapper` class\n",
    "\n",
    "All trained readout's coefficients / weights ($\\mathbf{W}^{\\mathrm{out}}$) are held in a separate class which stores the weights to memory, and also contains some analysis and plotting routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get_connections',\n",
       " 'measure_stability',\n",
       " 'plot_connections',\n",
       " 'plot_distributions',\n",
       " 'plot_spectral_radius',\n",
       " 'plot_weights',\n",
       " 'save',\n",
       " 'set_connections',\n",
       " 'update_weights']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fna.decoders.output_mapper import OutputMapper\n",
    "\n",
    "[func for func in dir(OutputMapper) if callable(getattr(OutputMapper, func)) and not func.startswith(\"__\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_parameters = {\n",
    "    'readout_E_ridge': {'algorithm': 'ridge', 'extractor': 'E_Vm_@offset', 'save': True},\n",
    "    'readout_E_pinv': {'algorithm': 'pinv', 'extractor': 'E_Vm_@offset', 'save': True},\n",
    "    'readout_E_logistic': {'algorithm': 'logistic', 'extractor': 'E_Vm_@offset', 'save': True},\n",
    "    'readout_E_perceptron': {'algorithm': 'perceptron', 'extractor': 'E_Vm_@offset', 'save': True},\n",
    "    'readout_E_svm-linear': {'algorithm': 'svm-linear', 'extractor': 'E_Vm_@offset', 'save': True},\n",
    "    'readout_E_svm-rbf': {'algorithm': 'svm-rbf', 'extractor': 'E_Vm_@offset', 'save': True},\n",
    "    'readout_E_force': {'algorithm': 'force', 'extractor': 'E_Vm_@offset', 'save': True},\n",
    "    'readout_E_reg': {'algorithm': 'sgd-reg', 'extractor': 'E_Vm_@offset', 'save': True},\n",
    "    'readout_E_class': {'algorithm': 'sgd-class', 'extractor': 'E_Vm_@offset', 'save': True},\n",
    "    'readout_E_ridge-sgd': {'algorithm': 'ridge-sgd', 'extractor': 'E_Vm_@offset', 'save': True},\n",
    "    'readout_E_pinv-sgd': {'algorithm': 'pinv-sgd', 'extractor': 'E_Vm_@offset', 'save': True},\n",
    "    'readout_E_log-sgd': {'algorithm': 'logistic-sgd', 'extractor': 'E_Vm_@offset', 'save': True},\n",
    "    'readout_E_svm-sgd': {'algorithm': 'svm-linear-sgd', 'extractor': 'E_Vm_@offset', 'save': True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "n_batches = 10\n",
    "batch_size = T\n",
    "continuous = True\n",
    "stim_onset = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Batch processing:**\n",
    "1. Parse an initial batch and discard it (inputs and responses) - discard initial transients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sequences.py:161 - INFO] Generating a random sequence of length 10, from a set of 10 symbols\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 10 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch transient\n"
     ]
    }
   ],
   "source": [
    "# Transient set\n",
    "batch_sequence = sequencer.generate_random_sequence(T=10)\n",
    "batch_stim_seq = signal.draw_stimulus_sequence(batch_sequence, onset_time=stim_onset, continuous=continuous, intervals=None)\n",
    "snn.process_batch(batch_label='transient', encoder=encoder, stim_seq=batch_stim_seq)\n",
    "stim_onset = snn.next_onset_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Training phase - Training proceeds in batches, with the readouts tuned after each batch. After training, a validation is run on the same batch to quantify the evolution of the readout tuning process and how it might reflect the underlying dynamics (particularly relevant in systems that change with time).\n",
    "\n",
    "**Note:** in ANNs, training typically also involves adjusting the internal connection weights as requires a single target (see [learning]()). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sequences.py:161 - INFO] Generating a random sequence of length 100, from a set of 10 symbols\n",
      "[sequences.py:161 - INFO] Generating a random sequence of length 100, from a set of 10 symbols\n",
      "[sequences.py:161 - INFO] Generating a random sequence of length 100, from a set of 10 symbols\n",
      "[sequences.py:161 - INFO] Generating a random sequence of length 100, from a set of 10 symbols\n",
      "[sequences.py:161 - INFO] Generating a random sequence of length 100, from a set of 10 symbols\n",
      "[sequences.py:161 - INFO] Generating a random sequence of length 100, from a set of 10 symbols\n",
      "[sequences.py:161 - INFO] Generating a random sequence of length 100, from a set of 10 symbols\n",
      "[sequences.py:161 - INFO] Generating a random sequence of length 100, from a set of 10 symbols\n",
      "[sequences.py:161 - INFO] Generating a random sequence of length 100, from a set of 10 symbols\n",
      "[sequences.py:161 - INFO] Generating a random sequence of length 100, from a set of 10 symbols\n"
     ]
    }
   ],
   "source": [
    "# generate training batches\n",
    "batch_labels = ['train_batch={}'.format(batch+1) for batch in range(n_batches)]\n",
    "batch_seq = [sequencer.generate_random_sequence(T=batch_size) for _ in range(n_batches)]\n",
    "batch_targets = [sequencer.generate_default_outputs(batch_sequence, max_memory=0, max_prediction=0,\n",
    "                    max_chunk=0, chunk_memory=False, chunk_prediction=False) for batch_sequence in batch_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[extractors.py:490 - INFO] Connecting extractors (SpikingExtractor):\n",
      "[extractors.py:496 - INFO] \t- State [E_Vm_@offset] from populations E [V_m]\n",
      "[extractors.py:309 - INFO] \t  - Extractor (offset sampling)\n",
      "[extractors.py:310 - INFO] \t\t- offset = 550.2 ms (stimulus onset + total delay + sim_res step)\n",
      "[extractors.py:311 - INFO] \t\t- interval = 50.0 ms\n",
      "[extractors.py:529 - INFO] \t\t- total delays = 0.2 ms (0.1 encoder + 0.1 extractor delay)\n",
      "[extractors.py:530 - INFO] \t\t- NEST device id(s): [203]\n"
     ]
    }
   ],
   "source": [
    "# create and connect extractors and decoders\n",
    "snn.connect_state_extractors(extractor_parameters, encoder=encoder, input_mapper=in_to_snn_connections,\n",
    "                             stim_duration=signal_pars['duration'], stim_isi=None, stim_onset=stim_onset,\n",
    "                             to_memory=True)\n",
    "snn.create_decoder(decoding_parameters)\n",
    "total_delay = in_to_snn_connections.total_delay + snn.state_extractor.max_extractor_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:79 - INFO] Initializing and connecting decoder...\n",
      "[__init__.py:86 - INFO] Creating readouts:\n",
      "[readouts.py:48 - INFO]   - readout_E_ridge trained with ridge on classification, using state E_Vm_@offset\n",
      "[readouts.py:48 - INFO]   - readout_E_pinv trained with pinv on classification, using state E_Vm_@offset\n",
      "[readouts.py:48 - INFO]   - readout_E_logistic trained with logistic on classification, using state E_Vm_@offset\n",
      "[readouts.py:48 - INFO]   - readout_E_perceptron trained with perceptron on classification, using state E_Vm_@offset\n",
      "[readouts.py:48 - INFO]   - readout_E_svm-linear trained with svm-linear on classification, using state E_Vm_@offset\n",
      "[readouts.py:48 - INFO]   - readout_E_svm-rbf trained with svm-rbf on classification, using state E_Vm_@offset\n",
      "[readouts.py:48 - INFO]   - readout_E_elast trained with elastic on classification, using state E_Vm_@offset\n",
      "[readouts.py:48 - INFO]   - readout_E_bayes trained with bayesian_ridge on classification, using state E_Vm_@offset\n",
      "[readouts.py:48 - INFO]   - readout_E_force trained with force on classification, using state E_Vm_@offset\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=1_epoch=1\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=2_epoch=1\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=1_epoch=1\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=1_epoch=1\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [550.1 600.1 650.1] ... [5500.099999999999]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [550.2 600.2 650.2] ... [5500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[readouts.py:195 - INFO] Performing 5-fold CV for logistic regression...\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[readouts.py:221 - INFO] Performing 5-fold CV for svm-rbf hyperparameters...\n",
      "/home/neuro/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning:The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 13693.90it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=1\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=1_epoch=1\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=2_epoch=1\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=3_epoch=1\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=2_epoch=1\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=2_epoch=1\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [5550.1 5600.1 5650.1] ... [10500.1]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [5550.2 5600.2 5650.2] ... [10500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 28221.67it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=1\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=2_epoch=1\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=3_epoch=1\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=4_epoch=1\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=3_epoch=1\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=3_epoch=1\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [10550.1 10600.1 10650.1] ... [15500.1]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [10550.2 10600.2 10650.2] ... [15500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 5526.89it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=1\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=3_epoch=1\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=4_epoch=1\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=5_epoch=1\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=4_epoch=1\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=4_epoch=1\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [15550.1 15600.1 15650.1] ... [20500.100000000002]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [15550.2 15600.2 15650.2] ... [20500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 13698.37it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=1\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=4_epoch=1\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=5_epoch=1\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=6_epoch=1\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=5_epoch=1\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=5_epoch=1\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [20550.1 20600.1 20650.1] ... [25500.100000000002]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [20550.2 20600.2 20650.2] ... [25500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 9981.45it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=1\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=5_epoch=1\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=6_epoch=1\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=7_epoch=1\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=6_epoch=1\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=6_epoch=1\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [25550.1 25600.1 25650.1] ... [30500.100000000002]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [25550.2 25600.2 25650.2] ... [30500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 5239.02it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=1\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=6_epoch=1\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=7_epoch=1\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=8_epoch=1\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=7_epoch=1\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=7_epoch=1\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [30550.1 30600.1 30650.1] ... [35500.100000000006]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [30550.2 30600.2 30650.2] ... [35500.200000000004]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 6771.78it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=1\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=7_epoch=1\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=8_epoch=1\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=9_epoch=1\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=8_epoch=1\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=8_epoch=1\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [35550.1 35600.1 35650.1] ... [40500.100000000006]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [35550.2 35600.2 35650.2] ... [40500.200000000004]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 10114.07it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=1\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=8_epoch=1\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=9_epoch=1\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=10_epoch=1\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=9_epoch=1\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=9_epoch=1\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [40550.1 40600.1 40650.1] ... [45500.100000000006]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [40550.2 40600.2 40650.2] ... [45500.200000000004]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 5924.66it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=1\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=9_epoch=1\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=10_epoch=1\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=1_epoch=2\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=10_epoch=1\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=10_epoch=1\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [45550.1 45600.1 45650.1] ... [50500.100000000006]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [45550.2 45600.2 45650.2] ... [50500.200000000004]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=1\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 12347.08it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=1\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=10_epoch=1\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=1_epoch=2\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=2_epoch=2\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=1_epoch=2\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=1_epoch=2\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [50550.1 50600.1 50650.1] ... [55500.100000000006]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [50550.2 50600.2 50650.2] ... [55500.200000000004]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 11763.58it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=1_epoch=2\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=1_epoch=2\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=2_epoch=2\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=3_epoch=2\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=2_epoch=2\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=2_epoch=2\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [55550.1 55600.1 55650.1] ... [60500.100000000006]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [55550.2 55600.2 55650.2] ... [60500.200000000004]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 18206.82it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=2_epoch=2\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=2_epoch=2\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=3_epoch=2\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=4_epoch=2\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=3_epoch=2\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=3_epoch=2\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [60550.1 60600.1 60650.1] ... [65500.100000000006]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [60550.2 60600.2 60650.2] ... [65500.200000000004]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 8633.28it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=3_epoch=2\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=3_epoch=2\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=4_epoch=2\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=5_epoch=2\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=4_epoch=2\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=4_epoch=2\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [65550.1 65600.1 65650.1] ... [70500.09999999999]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [65550.2 65600.2 65650.2] ... [70500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 10023.43it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=4_epoch=2\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=4_epoch=2\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=5_epoch=2\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=6_epoch=2\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=5_epoch=2\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=5_epoch=2\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [70550.1 70600.1 70650.1] ... [75500.09999999999]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [70550.2 70600.2 70650.2] ... [75500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 7265.88it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=5_epoch=2\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=5_epoch=2\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=6_epoch=2\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=7_epoch=2\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=6_epoch=2\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=6_epoch=2\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [75550.1 75600.1 75650.1] ... [80500.09999999999]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [75550.2 75600.2 75650.2] ... [80500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 9129.96it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=6_epoch=2\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=6_epoch=2\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=7_epoch=2\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=8_epoch=2\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=7_epoch=2\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=7_epoch=2\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [80550.1 80600.1 80650.1] ... [85500.09999999999]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [80550.2 80600.2 80650.2] ... [85500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 8240.28it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=7_epoch=2\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=7_epoch=2\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=8_epoch=2\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=9_epoch=2\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=8_epoch=2\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=8_epoch=2\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [85550.1 85600.1 85650.1] ... [90500.09999999999]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [85550.2 85600.2 85650.2] ... [90500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 8362.85it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=8_epoch=2\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=8_epoch=2\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=9_epoch=2\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:779 - INFO] Initializing training batch train_batch=10_epoch=2\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=9_epoch=2\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=9_epoch=2\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [90550.1 90600.1 90650.1] ... [95500.09999999999]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [90550.2 90600.2 90650.2] ... [95500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 5151.25it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=9_epoch=2\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=9_epoch=2\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch train_batch=10_epoch=2\n",
      "[__init__.py:739 - INFO] Simulating delays from batch train_batch=10_epoch=2\n",
      "[__init__.py:743 - INFO] Post-processing batch train_batch=10_epoch=2\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 203 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [203] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[extractors.py:905 - INFO] \n",
      "[extractors.py:906 - INFO] ==========================\n",
      "[extractors.py:907 - INFO] SNN StateExtractor summary\n",
      "[extractors.py:909 - INFO] ---------------\n",
      "[extractors.py:911 - INFO] State extractor [E_Vm_@offset] from population(s) E with gid(s) [203]\n",
      "[extractors.py:931 - INFO]   - gathered 100 samples @offset, sampling from V_m, extractor delay 0.1 ms\n",
      "[extractors.py:933 - INFO]     - network time (incl. encoder delay):\n",
      "[extractors.py:934 - INFO]       [95550.1 95600.1 95650.1] ... [100500.09999999999]\n",
      "[extractors.py:935 - INFO]     - NEST kernel time (incl. all delays):\n",
      "[extractors.py:937 - INFO]       [95550.2 95600.2 95650.2] ... [100500.2]\n",
      "[extractors.py:938 - INFO] \n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [pinv] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [logistic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [perceptron] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-linear] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [svm-rbf] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [elastic] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [bayesian_ridge] on [classification] task\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=2\n",
      "[__init__.py:153 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[readouts.py:169 - INFO]   - Training readout with [force] on [classification] task\n",
      "Online FORCE learning : 100%|██████████| 100/100 [00:00<00:00, 8539.07it/s]\n",
      "[__init__.py:260 - INFO] Updating OutputMapper with data from batch train_batch=10_epoch=2\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:754 - INFO] Finished training batch train_batch=10_epoch=2\n",
      "[__init__.py:300 - WARNING] Could not save OutputMapper readout_E_ridge, storage paths not set?\n",
      "[__init__.py:300 - WARNING] Could not save OutputMapper readout_E_pinv, storage paths not set?\n",
      "[__init__.py:300 - WARNING] Could not save OutputMapper readout_E_logistic, storage paths not set?\n",
      "[__init__.py:300 - WARNING] Could not save OutputMapper readout_E_perceptron, storage paths not set?\n",
      "[__init__.py:300 - WARNING] Could not save OutputMapper readout_E_svm-linear, storage paths not set?\n",
      "[__init__.py:300 - WARNING] Could not save OutputMapper readout_E_svm-rbf, storage paths not set?\n",
      "[__init__.py:300 - WARNING] Could not save OutputMapper readout_E_elast, storage paths not set?\n",
      "[__init__.py:300 - WARNING] Could not save OutputMapper readout_E_bayes, storage paths not set?\n",
      "[__init__.py:300 - WARNING] Could not save OutputMapper readout_E_force, storage paths not set?\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "for epoch in range(n_epochs):\n",
    "    for batch in range(n_batches):\n",
    "        batch_label = '{}_epoch={}'.format(batch_labels[batch], epoch+1)\n",
    "        batch_input = signal.draw_stimulus_sequence(batch_seq[batch], onset_time=stim_onset,continuous=continuous, intervals=None)\n",
    "\n",
    "        snn.train(batch_label, n_batches*n_epochs, stim_seq=batch_input, encoder=encoder, target_outputs=batch_targets[batch], total_delay=total_delay)\n",
    "\n",
    "        stim_onset = snn.next_onset_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Test phase - To evaluate the readouts' performance, we process a test batch and measure its ability to predict the target using only the system states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sequences.py:161 - INFO] Generating a random sequence of length 100, from a set of 10 symbols\n",
      "[embeddings.py:655 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[embeddings.py:605 - INFO] Concatenating stimulus sequence\n",
      "[__init__.py:825 - INFO] Simulating test set fna_test_set\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[extractors.py:540 - INFO] Reinitializing extractor [E_Vm_@offset]\n",
      "[extractors.py:543 - INFO]   - Stopping recording from device [203]\n",
      "[extractors.py:309 - INFO] \t  - Extractor (offset sampling)\n",
      "[extractors.py:310 - INFO] \t\t- offset = 100550.5 ms (stimulus onset + total delay + sim_res step)\n",
      "[extractors.py:311 - INFO] \t\t- interval = 50.0 ms\n",
      "[__init__.py:695 - INFO] \n",
      "[__init__.py:696 - INFO] ################## Simulating batch fna_test_set\n",
      "[extractors.py:805 - INFO] Extracting and storing recorded activity from state extractor E_Vm_@offset\n",
      "[extractors.py:842 - INFO]   - Reading extractor 204 [V_m] at sampling offset\n",
      "[extractors.py:584 - INFO]   - Flushing devices of state extractor E_Vm_@offset [204] from populations V_m\n",
      "[extractors.py:884 - INFO] Compiling state matrices...\n",
      "[__init__.py:349 - WARNING] Could not save StateMatrix E_Vm_@offset, storage paths not set?\n",
      "[__init__.py:188 - INFO] Processing readout [readout_E_ridge] with state [E_Vm_@offset]\n",
      "[readouts.py:286 - INFO]   - Testing readout with [ridge] on task [classification]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[__init__.py:188 - INFO] Processing readout [readout_E_pinv] with state [E_Vm_@offset]\n",
      "[readouts.py:286 - INFO]   - Testing readout with [pinv] on task [classification]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[__init__.py:188 - INFO] Processing readout [readout_E_logistic] with state [E_Vm_@offset]\n",
      "[readouts.py:286 - INFO]   - Testing readout with [logistic] on task [classification]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[__init__.py:188 - INFO] Processing readout [readout_E_perceptron] with state [E_Vm_@offset]\n",
      "[readouts.py:286 - INFO]   - Testing readout with [perceptron] on task [classification]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[__init__.py:188 - INFO] Processing readout [readout_E_svm-linear] with state [E_Vm_@offset]\n",
      "[readouts.py:286 - INFO]   - Testing readout with [svm-linear] on task [classification]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[__init__.py:188 - INFO] Processing readout [readout_E_svm-rbf] with state [E_Vm_@offset]\n",
      "[readouts.py:286 - INFO]   - Testing readout with [svm-rbf] on task [classification]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[__init__.py:188 - INFO] Processing readout [readout_E_elast] with state [E_Vm_@offset]\n",
      "[readouts.py:286 - INFO]   - Testing readout with [elastic] on task [classification]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[__init__.py:188 - INFO] Processing readout [readout_E_bayes] with state [E_Vm_@offset]\n",
      "[readouts.py:286 - INFO]   - Testing readout with [bayesian_ridge] on task [classification]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[__init__.py:188 - INFO] Processing readout [readout_E_force] with state [E_Vm_@offset]\n",
      "[readouts.py:286 - INFO]   - Testing readout with [force] on task [classification]\n",
      "[embeddings.py:28 - INFO] Creating symbolic embeddings\n",
      "[embeddings.py:91 - INFO] - 10 symbols encoded as one-hot\n",
      "[embeddings.py:219 - INFO] Generating stimulus sequence: 100 symbols\n",
      "[extractors.py:592 - INFO] Deleting activity data from all state extractors\n",
      "[__init__.py:857 - INFO] Finished test set fna_test_set\n",
      "[readouts.py:305 - INFO] Evaluating readout_E_ridge performance...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 1 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ddf1ce3d5b38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_stim_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_delay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_parsing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k-WTA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbolic_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/code/func-neurarch/networks/snn/__init__.py\u001b[0m in \u001b[0;36mdecoder_accuracy\u001b[0;34m(self, output_parsing, symbolic_task)\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \"\"\"\n\u001b[0;32m--> 866\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_output_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_parsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbolic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msymbolic_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/code/func-neurarch/decoders/__init__.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, process_output_method, symbolic, flush)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mreadout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadouts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0mr_perf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_output_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_output_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbolic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msymbolic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m                 \u001b[0mperfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr_perf\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/code/func-neurarch/decoders/readouts.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, process_output_method, symbolic)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msymbolic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mbinary_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_output_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0mbinary_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/code/func-neurarch/decoders/readouts.py\u001b[0m in \u001b[0;36mparse_outputs\u001b[0;34m(output, method, k)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mbinary_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'threshold'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 1 with size 10"
     ]
    }
   ],
   "source": [
    "test_size = 100\n",
    "test_label = 'fna_test_set'\n",
    "test_sequence = sequencer.generate_random_sequence(T=test_size)\n",
    "target_outputs = sequencer.generate_default_outputs(test_sequence, max_memory=0, max_chunk=0, max_prediction=0,\n",
    "                                                    chunk_memory=False, chunk_prediction=False)\n",
    "test_stim_seq = signal.draw_stimulus_sequence(test_sequence, onset_time=stim_onset,\n",
    "                                                       continuous=continuous, intervals=None)\n",
    "\n",
    "snn.predict(test_label, test_stim_seq, encoder, target_outputs, total_delay)\n",
    "snn.decoder_accuracy(output_parsing='k-WTA', symbolic_task=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[readouts.py:305 - INFO] Evaluating readout_E_ridge performance...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [100, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7c4aa42f2dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_parsing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbolic_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/code/func-neurarch/networks/snn/__init__.py\u001b[0m in \u001b[0;36mdecoder_accuracy\u001b[0;34m(self, output_parsing, symbolic_task)\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \"\"\"\n\u001b[0;32m--> 866\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_output_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_parsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbolic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msymbolic_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/code/func-neurarch/decoders/__init__.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, process_output_method, symbolic, flush)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mreadout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadouts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0mr_perf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_output_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_output_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbolic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msymbolic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m                 \u001b[0mperfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr_perf\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/code/func-neurarch/decoders/readouts.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, process_output_method, symbolic)\u001b[0m\n\u001b[1;32m    329\u001b[0m             self.performance.update({\n\u001b[1;32m    330\u001b[0m                 'label': {\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0;34m'hamming_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhamming_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m'precision'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_precision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [100, 10]"
     ]
    }
   ],
   "source": [
    "snn.decoder_accuracy(output_parsing=None, symbolic_task=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "func-neurarch",
   "language": "python",
   "name": "func-neurarch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
