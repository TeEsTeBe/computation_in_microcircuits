{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Overview**\n",
    "The work discussed here attempts to provide bridges between AI, neuroscience and cognitive science. A primary goal of the approach is to study functional neurodynamics in a systematic manner across different neural network architectures, akin to e.g. [[1](https://arxiv.org/abs/1907.08549)], [[2](http://arxiv.org/abs/1907.08549)], but extended to the domain of *biologically compatible architectures* and *cognitively compatible computations*. By employing direct and systematic comparisons between biologically plausible spiking neural network (SNN) models and state-of-the-art artificial neural networks (ANN), we can gain insights into the nature and types of solutions that different systems find for the same problem domains.\n",
    "\n",
    "**Objectives:**\n",
    "* systematically compare and decompose cognitive computation in neurobiological systems\n",
    "* evaluate, side-by-side, systems with various degrees of complexity and biophysical compatibility performing similar, cognitively-inspired computations\n",
    "* benchmark the systems' properties and quantify the nature and qualities of the solutions \n",
    "* establishing parallels with human behavioral performance in similar tasks\n",
    "\n",
    "---\n",
    "\n",
    "For convenience, we divide the problem into 2 classes of models:\n",
    "\n",
    "**A) Function models** refer to task specifications and computational requirements and will be primarily inspired by psycholinguistic research, but also cognitive and behavioral sciences and computer science. \n",
    "\n",
    "**B) System models** refer to the network architectures; from simple, standard recurrent networks of sigmoid neurons (ESN, *Vanilla* RNN), to complex models of layered cortical microcircuits; from fully supervised to unsupervised and reinforcement learning.\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "\n",
    "[[1](https://arxiv.org/abs/1907.08549)] - Maheswaranathan, N., Williams, A. H., Golub, M. D., Ganguli, S., & Sussillo, D. (2019). Universality and individuality in neural dynamics across large populations of recurrent networks.\n",
    "\n",
    "[[2](https://www.nature.com/articles/s41593-018-0310-2)] - Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T., & Wang, X. J. (2019). Task representations in neural networks trained to perform many cognitive tasks. Nature Neuroscience, 22(2), 297–306. \n",
    "\n",
    "[[3](https://www.ncbi.nlm.nih.gov/pubmed/21466123)] - Eliasmith, C. (2010). How we ought to describe computation in the brain. Studies in History and Philosophy of Science Part A, 41(3), 313–320.\n",
    "\n",
    "[[4](https://www.cell.com/current-biology/fulltext/S0960-9822(19)30204-0?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0960982219302040%3Fshowall%3Dtrue)] - Kriegeskorte, N., & Golan, T. (2019). Neural network models and deep learning. Current Biology, 29(7), R231–R236. \n",
    "\n",
    "[[5](https://www.frontiersin.org/articles/10.3389/fncom.2016.00094/full)] - Marblestone AH, Wayne G and Kording KP (2016) Toward an Integration of Deep Learning and Neuroscience. Front. Comput. Neurosci. 10:94.\n",
    "\n",
    "[[6](https://www.nature.com/articles/s41593-019-0520-2)] - Richards, B. A., Lillicrap, T. P., Beaudoin, P., Bengio, Y., Bogacz, R., Christensen, A., … Kording, K. P. (2019). A deep learning framework for neuroscience. Nature Neuroscience, 22(11), 1761–1770. \n",
    "\n",
    "[[7](https://doi.org/10.1016/j.neuron.2017.06.011)] - Hassabis, D., Kumaran, D., Summerfield, C., & Botvinick, M. (2017). Neuroscience-Inspired Artificial Intelligence. Neuron, 95, 245–258. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Table of Contents**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## **A) Function models - the tasks**\n",
    "\n",
    "### 1. [Symbolic input sequences](symbolic_sequences.ipynb#section1)\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;1.1. The `SymbolicSequencer` base class and tools\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;1.2. Designing sequences with tokens and transition tables - `ArtificialGrammar`\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;1.3. Sequences with variable non-adjacent dependencies - `NonAdjacentDependencies`\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;1.4. Continuous working memory - `12AX`\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;1.5. Natural language data - `NaturalLanguage` \n",
    "\n",
    "### 2. [Evaluate sequences - complexity and token frequency](symbolic_sequences.ipynb#section2)\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;2.1. Frequency distributions and set sizes\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;2.2. String-set complexity \n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;2.3. Sequence complexity\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;2.4. Topographical Entropy\n",
    "\n",
    "### 3. [Discrete symbolic embeddings](embeddings.ipynb#section3) - tokens to vectors\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;3.1. `VectorEmbeddings` - discrete inputs\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;3.2. `Word2VecEmbedding` \n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;3.3. Embedding complexity and input space geometry - discrete input spaces\n",
    "\n",
    "### 4. [Continuous symbolic embeddings](embeddings.ipynb#section4) - tokens to signals\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;4.1. `DynamicEmbeddings` - continuous inputs\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.1.1. Spatiotemporal Spike Patterns (`frozen noise`)\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.1.2. Unfolding vector embeddings into continuous signals \n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.1.3. Unfolding vector embeddings into spikes \n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.1.4. Sequencing dynamic embeddings\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.1.5. Variability in duration, amplitude and ISI\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;4.2. Embedding complexity and input trajectories - continuous input spaces\n",
    "\n",
    "### 5. [*Sensory* frontends](embeddings.ipynb#section5) - processing real-world data\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;5.1. `ImageFrontend`\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;5.2. `AudioFrontend`\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;5.3. `VideoFrontend`\n",
    "\n",
    "### 6. [Stimulus encoding](encodings.ipynb) - delivering inputs to networks\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;6.1. `NESTEncoder`\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;6.2. `InputMapper`\n",
    "\n",
    "### 7. [Complete task examples](function_models.ipynb)\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;7.1. Symbolic mappings\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.1.1. Default sequence mappings\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.1.2. Default string-level mappings\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.1.3. Invariant recognition and cross-modal generalization\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;7.2. Cognitive / Behavioral tasks and experimental paradigms\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.2.1. The Elman language\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.2.2. Artificial Grammar Learning (AGL)\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.2.3. Learning Non-adjacent dependencies\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.2.4. Context-free and context-sensitive languages\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.2.5. Continuous memorization (1-2-A-X)\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;7.3. Cognitive / Behavioral experimental paradigms\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.3.1. Delayed match-to-sample - **working memory**\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.3.2. Deviant detection (*odd-ball* paradigms) - **attention, working memory, prediction**\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.3.3. Blind source separation (*cocktail party problem*) - **attention**\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.3.4. Trail-making test - **executive function, decision-making**\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.3.5. Stroop interference task - **executive function**\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.3.6. Porteus maze - **executive function, decision-making**\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.3.7. Wiskonsin card-sorting task - **decision-making**\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.3.8. The flash-lag effect - **attention, sensory memory**\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.3.9. Visuospatial navigation (...)\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;7.4. Natural Language Processing/Understanding (NLP)\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.4.1. Semantic dependency parsing - role labelling, chunking, word sense disambiguation\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.4.2. POS tagging - using annottated corpora - (...)\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;7.5. Analog processing tasks\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.1. Information processing capacity\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.2. Continuous integration\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.3. Temporal XOR\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.4. Nonlinear autoregressive moving average (NARMA)\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.5. Mackey-Glass chaotic time series (MG) - (...and other chaotic ts prediction)\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.6. Logistic map\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.7. n-bit flip-flop\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.8. Pattern generation\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.9. (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **B) System models - the architectures**\n",
    "\n",
    "### 8. [Task Performance and Learning](system_models.ipynb)\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;8.1. Task specifications\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8.1.1. Preparing data batches\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;8.2. Input-driven dynamics\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;8.3. Supervised optimization problems\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;8.4. System optimization\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8.4.1. Supervised\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8.4.2. Self-supervised\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8.4.3. Unsupervised and reward-modulated\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;8.5. Decoder optimization\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;8.6. Preparing data batches\n",
    "\n",
    "\n",
    "### 9. [Artificial Neural Networks](system_models_ANN.ipynb) (`ArtificialNeuralNetwork`)\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;9.1. Implemented Models\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;9.2. Training ANNs\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9.2.1. *Symbolic* Task\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9.2.2. *Analog* Task\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;9.3. Decoding ANNs\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;9.4. Profiling ANNs\n",
    "\n",
    "\n",
    "### 10. [Continuous Rate Networks](system_models_cRNN.ipynb) (`ContinuousRateNetwork` and `ReservoirRecurrentNetwork`)\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;10.1. Implemented Models\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;10.2. Training ANNs\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10.2.1. *Symbolic* Task\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10.2.2. *Analog* Task\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;10.3. Decoding ANNs\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;10.4. Profiling ANNs\n",
    "\n",
    "\n",
    "### 11. [Spiking Neural Networks](system_models_SNN.ipynb) (`SpikingNeuralNetwork`, `BiophysicalSpikingNetwork`)\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;11.1. Creating SNNs - `SpikingNetwork`\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;11.2. Connecting SNNs - `NESTConnector`\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;11.3. Profiling SNNs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **C) Decoding and functional analysis**\n",
    "\n",
    "### 12. [State extraction and sampling](state_sampling.ipynb#section11) - gathering population responses\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;12.1. Fixed and constant signal duration and ISIs \n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.1.1. Sampling at a fixed rate\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.1.2. Sampling at fixed time points\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.1.3. Creating and connecting state extractors\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.1.4. Simulate and plot\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.1.5. Consistency checks\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.1.6. Resulting state matrices\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;12.2. Variable signal durations and ISIs \n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.2.1. Sampling at a fixed rate\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.2.2. Sampling at fixed time points\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.2.3. Creating and connecting state extractors\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.2.4. Simulate and plot\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.2.5. Consistency checks\n",
    "\n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.2.6. Resulting state matrices\n",
    "\n",
    "\n",
    "### 13. [Decoding and Readout](decoding.ipynb) - processing multiple information streams in parallel  \n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;13.1. The `Readout`\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;13.2. Performance\n",
    "\n",
    "<!-- \n",
    "## 13. Task performance and functional geometry - `StateAnalyst`\n",
    "\n",
    "### 13.1. Latent structure\n",
    "\n",
    "### 13.2.1. Dimensionality reduction and embedding models\n",
    "\n",
    "### 11.2.2. Hidden Markov Models\n",
    "\n",
    "### 11.2.3. Manifolds / Fibrations ...\n",
    "\n",
    "### 11.2.4. Information content and Active Information Storage (AIS) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
